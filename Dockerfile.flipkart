FROM python:3.10-slim

# Install system dependencies and Google Chrome in a single layer for better caching
RUN apt-get update && apt-get install -y --no-install-recommends \
    wget \
    gnupg \
    unzip \
    curl \
    xvfb \
    ca-certificates \
    libnss3 \
    libxss1 \
    libasound2 \
    libatk-bridge2.0-0 \
    libgtk-3-0 \
    libgbm1 \
    libx11-xcb1 \
    libxcomposite1 \
    libxcursor1 \
    libxdamage1 \
    libxi6 \
    libxtst6 \
    libxrandr2 \
    libxrender1 \
    libdrm2 \
    xdg-utils \
    fonts-liberation \
    && wget -qO- https://dl.google.com/linux/linux_signing_key.pub | gpg --dearmor > /usr/share/keyrings/google.gpg \
    && echo "deb [signed-by=/usr/share/keyrings/google.gpg arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" > /etc/apt/sources.list.d/google-chrome.list \
    && apt-get update \
    && apt-get install -y --no-install-recommends google-chrome-stable \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Set up Chrome environment variables
ENV CHROME_BIN=/usr/bin/google-chrome
ENV CHROME_PATH=/usr/bin/google-chrome

# Set working directory
WORKDIR /app

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies in a single layer
RUN pip install --no-cache-dir -r requirements.txt \
    && pip install --no-cache-dir pandas

# Copy the Flipkart scraper and data files
COPY flipkart_search_and_extract.py .
COPY expanded_permutations.csv .

# Create directories for outputs in a single layer
RUN mkdir -p screenshots data

# Set environment variables for the scraper
ENV PYTHONUNBUFFERED=1
ENV PYTHONPATH=/app

# =============================================================================
# MEMORY OPTIMIZATION SETTINGS
# =============================================================================
# These settings are optimized for running the Flipkart scraper in Docker
# containers with limited memory resources. They help prevent memory leaks
# and ensure efficient resource usage.

# Memory optimization settings for better performance in containers
ENV MALLOC_ARENA_MAX=2
ENV MALLOC_MMAP_THRESHOLD_=131072
ENV MALLOC_TRIM_THRESHOLD_=131072
ENV MALLOC_TOP_PAD_=131072
ENV MALLOC_MMAP_MAX_=65536

# Python memory optimization
ENV PYTHONHASHSEED=0
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# Chrome optimization for containerized environment
ENV CHROME_NO_SANDBOX=true
ENV CHROME_DISABLE_GPU=true
ENV CHROME_DISABLE_DEV_SHM_USAGE=true
ENV CHROME_DISABLE_BACKGROUND_TIMER_THROTTLING=true
ENV CHROME_DISABLE_RENDERER_BACKGROUNDING=true
ENV CHROME_DISABLE_BACKGROUNDING_OCCLUDED_WINDOWS=true

# Docker-specific environment variables for thread management
ENV DOCKER_CONTAINER=true
ENV container=docker

# Memory limits for Chrome (in MB)
ENV CHROME_MEMORY_LIMIT=2048
ENV CHROME_MAX_OLD_SPACE_SIZE=2048

# Additional memory optimization for containerized environment
ENV MALLOC_CHECK_=0
ENV MALLOC_PERTURB_=0

# Set memory limits for the container (can be overridden at runtime)
ENV MEMORY_LIMIT=4G
ENV SWAP_LIMIT=2G

# Thread and process limits
ENV MAX_THREADS=100
ENV MAX_PROCESSES=50

# Expose port if needed for future API functionality
EXPOSE 5000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD python3 -c "import os; exit(0 if os.path.exists('flipkart_scraping_progress.json') else 1)"

# Default command - run the Flipkart scraper in headless mode
CMD ["python3", "flipkart_search_and_extract.py", "--headless"]
